{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429baf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aded26",
   "metadata": {},
   "source": [
    "# 1. 自定义数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d768624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwrittenDigitsDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations_dir, transform=None):\n",
    "        self.img_names = [img for img in os.listdir(img_dir) if img.endswith('.jpg')]\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        image = read_image(img_path).float()  # 这里已经是Tensor了\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # 确保transform不包含ToTensor()\n",
    "\n",
    "        # 加载注释并构造目标字典\n",
    "        annotation_file = os.path.join(self.annotations_dir, f\"{idx+1}.txt\")\n",
    "        boxes, labels = self.read_annotation(annotation_file)\n",
    "        \n",
    "        targets = {}\n",
    "        targets[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        targets[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)  \n",
    "\n",
    "        return image, targets\n",
    "\n",
    "    def read_annotation(self, file_path):\n",
    "        annotations = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                annotations.append(list(map(int, line.split())))\n",
    "\n",
    "        # 转换为 Tensor\n",
    "        annotations = torch.tensor(annotations, dtype=torch.int64)\n",
    "        # 分割标签和边界框\n",
    "        labels = annotations[:, 0]\n",
    "        boxes = annotations[:, 1:]\n",
    "        # boxes[:,  2:4] = boxes[:,0:2] + boxes[:, 2:4]\n",
    "        return boxes, labels\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((48, 96), antialias=True),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddff439",
   "metadata": {},
   "source": [
    "# 2. 预处理图像数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a364e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset):1618,train_size:1618,val_size:405\n"
     ]
    }
   ],
   "source": [
    "train_dataset = HandwrittenDigitsDataset(img_dir='../A+B/samples/inputs', annotations_dir='../A+B/samples/explanations', transform=transform)\n",
    "# train_dataset 是您的完整训练数据集\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))  # 80% 的数据用于训练\n",
    "val_size = len(train_dataset) - train_size  # 20% 的数据用于验证\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f'len(train_dataset):{len(train_dataset)},train_size:{train_size},val_size:{val_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6e8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, targets in val_loader:\n",
    "#     print(targets)\n",
    "#     break  # 只打印第一批数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d77a6c",
   "metadata": {},
   "source": [
    "# 3.模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeee22c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(48,), max_size=256, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=44, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "def get_model(num_classes):\n",
    "    # 加载预训练的模型\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # 获取分类器的输入特征数\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # 每个特征图层的锚框尺寸，每个特征图层的锚框长宽比\n",
    "    anchor_sizes = ((16,), (32,), (64,), (128,), (256,))\n",
    "    anchor_aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "    anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=anchor_aspect_ratios)\n",
    "    model.rpn.anchor_generator = anchor_generator\n",
    "    # 替换预训练的头部以适应新的类别数\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.transform.min_size = (48,)\n",
    "    model.transform.max_size = 256\n",
    "    return model\n",
    "\n",
    "# 由于您的任务是检测数字，类别数设置为10（0-9）加一个背景类\n",
    "model = get_model(num_classes=11)\n",
    "# 将模型移到GPU\n",
    "device = torch.device('cuda:2') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42cc60",
   "metadata": {},
   "source": [
    "# 4. 定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c4e57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc61846f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef evaluate(model, val_loader, device):\\n    model.eval()\\n    total_loss = 0\\n    with torch.no_grad():\\n        for images, target_dict in val_loader:\\n            images = [image.to(device) for image in images]\\n            targets = [{k: v[i].to(device) for k, v in target_dict.items()} for i in range(len(images))]\\n\\n            loss_dicts = model(images, targets)\\n\\n            # 对每个图像的损失进行处理\\n            for loss_dict in loss_dicts:\\n                # 将所有损失转换为相同长度\\n                loss_lengths = [len(loss) for loss in loss_dict.values()]\\n                max_length = max(loss_lengths)\\n                \\n                # 如果损失长度不一致，则进行填充\\n                for key in loss_dict.keys():\\n                    if len(loss_dict[key]) < max_length:\\n                        padding = torch.zeros(max_length - len(loss_dict[key]), device=device)\\n                        loss_dict[key] = torch.cat((loss_dict[key], padding))\\n                \\n                # 计算图像的总损失\\n                image_loss = sum(loss.sum() for loss in loss_dict.values())\\n                total_loss += image_loss.item()\\n\\n    # 计算平均损失\\n    return total_loss / len(val_loader)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets_list in val_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                # 提取预测类别\n",
    "                pred_labels = output['labels'].cpu().tolist()\n",
    "                all_preds.extend(pred_labels)\n",
    "\n",
    "                # 提取真实类别\n",
    "                if 'labels' in targets_list[i]:\n",
    "                    true_labels = targets_list[i]['labels'].type(torch.int64).cpu().tolist()\n",
    "                    all_labels.extend(true_labels)\n",
    "                else:\n",
    "                    raise TypeError(\"Target is not a dictionary or doesn't have 'labels' key.\")\n",
    "\n",
    "    # 计算精确度和召回率\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, target_dict in val_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v[i].to(device) for k, v in target_dict.items()} for i in range(len(images))]\n",
    "\n",
    "            loss_dicts = model(images, targets)\n",
    "\n",
    "            # 对每个图像的损失进行处理\n",
    "            for loss_dict in loss_dicts:\n",
    "                # 将所有损失转换为相同长度\n",
    "                loss_lengths = [len(loss) for loss in loss_dict.values()]\n",
    "                max_length = max(loss_lengths)\n",
    "                \n",
    "                # 如果损失长度不一致，则进行填充\n",
    "                for key in loss_dict.keys():\n",
    "                    if len(loss_dict[key]) < max_length:\n",
    "                        padding = torch.zeros(max_length - len(loss_dict[key]), device=device)\n",
    "                        loss_dict[key] = torch.cat((loss_dict[key], padding))\n",
    "                \n",
    "                # 计算图像的总损失\n",
    "                image_loss = sum(loss.sum() for loss in loss_dict.values())\n",
    "                total_loss += image_loss.item()\n",
    "\n",
    "    # 计算平均损失\n",
    "    return total_loss / len(val_loader)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a84ae",
   "metadata": {},
   "source": [
    "# 5. 编写训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f913f33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03b3f98e65b45098b0abf15ac2c3f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Initializing:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys:loss_classifier:315.929931640625\n",
      "keys:loss_box_reg:281.45904541015625\n",
      "keys:loss_objectness:254.41268920898438\n",
      "keys:loss_rpn_box_reg:3.1318278312683105\n",
      "keys:loss_classifier:80.72932434082031\n",
      "keys:loss_box_reg:116.24353790283203\n",
      "keys:loss_objectness:13.164202690124512\n",
      "keys:loss_rpn_box_reg:1.8705593347549438\n",
      "keys:loss_classifier:64.40179443359375\n",
      "keys:loss_box_reg:81.9325942993164\n",
      "keys:loss_objectness:15.28142261505127\n",
      "keys:loss_rpn_box_reg:1.4766595363616943\n",
      "keys:loss_classifier:54.019290924072266\n",
      "keys:loss_box_reg:60.453041076660156\n",
      "keys:loss_objectness:6.364625453948975\n",
      "keys:loss_rpn_box_reg:1.3207186460494995\n",
      "keys:loss_classifier:35.6097412109375\n",
      "keys:loss_box_reg:46.80824661254883\n",
      "keys:loss_objectness:4.8201003074646\n",
      "keys:loss_rpn_box_reg:1.2833175659179688\n",
      "keys:loss_classifier:21.294233322143555\n",
      "keys:loss_box_reg:32.489501953125\n",
      "keys:loss_objectness:4.810794830322266\n",
      "keys:loss_rpn_box_reg:1.1431610584259033\n",
      "keys:loss_classifier:16.16702651977539\n",
      "keys:loss_box_reg:24.046363830566406\n",
      "keys:loss_objectness:4.182069301605225\n",
      "keys:loss_rpn_box_reg:0.9099341630935669\n",
      "keys:loss_classifier:10.32274341583252\n",
      "keys:loss_box_reg:19.3792724609375\n",
      "keys:loss_objectness:2.885099411010742\n",
      "keys:loss_rpn_box_reg:0.6905261278152466\n",
      "keys:loss_classifier:5.776206970214844\n",
      "keys:loss_box_reg:9.90342903137207\n",
      "keys:loss_objectness:2.156834125518799\n",
      "keys:loss_rpn_box_reg:0.5184555053710938\n",
      "keys:loss_classifier:6.048858165740967\n",
      "keys:loss_box_reg:8.377113342285156\n",
      "keys:loss_objectness:0.7862713932991028\n",
      "keys:loss_rpn_box_reg:0.4144015312194824\n",
      "keys:loss_classifier:4.5075178146362305\n",
      "keys:loss_box_reg:7.882681846618652\n",
      "keys:loss_objectness:1.244226098060608\n",
      "keys:loss_rpn_box_reg:0.41616955399513245\n",
      "keys:loss_classifier:7.016392707824707\n",
      "keys:loss_box_reg:7.160441875457764\n",
      "keys:loss_objectness:1.0497498512268066\n",
      "keys:loss_rpn_box_reg:0.4544898569583893\n",
      "keys:loss_classifier:5.457167625427246\n",
      "keys:loss_box_reg:4.993833541870117\n",
      "keys:loss_objectness:1.3774745464324951\n",
      "keys:loss_rpn_box_reg:0.3542500138282776\n",
      "keys:loss_classifier:4.8129563331604\n",
      "keys:loss_box_reg:3.0739505290985107\n",
      "keys:loss_objectness:1.0226142406463623\n",
      "keys:loss_rpn_box_reg:0.22787582874298096\n",
      "keys:loss_classifier:3.6804699897766113\n",
      "keys:loss_box_reg:2.4802839756011963\n",
      "keys:loss_objectness:0.7659848928451538\n",
      "keys:loss_rpn_box_reg:0.18714481592178345\n",
      "keys:loss_classifier:4.195975303649902\n",
      "keys:loss_box_reg:3.3284366130828857\n",
      "keys:loss_objectness:0.5383707284927368\n",
      "keys:loss_rpn_box_reg:0.22112224996089935\n",
      "keys:loss_classifier:2.7190725803375244\n",
      "keys:loss_box_reg:2.665316104888916\n",
      "keys:loss_objectness:0.5892223715782166\n",
      "keys:loss_rpn_box_reg:0.2300076186656952\n",
      "keys:loss_classifier:1.5647258758544922\n",
      "keys:loss_box_reg:2.042806386947632\n",
      "keys:loss_objectness:0.5226966142654419\n",
      "keys:loss_rpn_box_reg:0.2222510427236557\n",
      "keys:loss_classifier:2.043513059616089\n",
      "keys:loss_box_reg:2.0638794898986816\n",
      "keys:loss_objectness:0.673581600189209\n",
      "keys:loss_rpn_box_reg:0.19871167838573456\n",
      "keys:loss_classifier:1.7667025327682495\n",
      "keys:loss_box_reg:1.3244695663452148\n",
      "keys:loss_objectness:0.4013489782810211\n",
      "keys:loss_rpn_box_reg:0.18334652483463287\n",
      "keys:loss_classifier:0.9073286056518555\n",
      "keys:loss_box_reg:1.3110101222991943\n",
      "keys:loss_objectness:0.6225026845932007\n",
      "keys:loss_rpn_box_reg:0.16834942996501923\n",
      "keys:loss_classifier:0.9556542634963989\n",
      "keys:loss_box_reg:1.1982940435409546\n",
      "keys:loss_objectness:0.3499242663383484\n",
      "keys:loss_rpn_box_reg:0.1535901576280594\n",
      "keys:loss_classifier:0.8286024928092957\n",
      "keys:loss_box_reg:0.8689664602279663\n",
      "keys:loss_objectness:0.4493522047996521\n",
      "keys:loss_rpn_box_reg:0.13823026418685913\n",
      "keys:loss_classifier:0.943206250667572\n",
      "keys:loss_box_reg:0.9732216596603394\n",
      "keys:loss_objectness:0.24642959237098694\n",
      "keys:loss_rpn_box_reg:0.13688433170318604\n",
      "keys:loss_classifier:0.8640369772911072\n",
      "keys:loss_box_reg:0.812696099281311\n",
      "keys:loss_objectness:0.3486972153186798\n",
      "keys:loss_rpn_box_reg:0.12938252091407776\n",
      "keys:loss_classifier:0.7869246006011963\n",
      "keys:loss_box_reg:0.7518919706344604\n",
      "keys:loss_objectness:0.2530664801597595\n",
      "keys:loss_rpn_box_reg:0.11836447566747665\n",
      "keys:loss_classifier:0.8145017623901367\n",
      "keys:loss_box_reg:0.6766347289085388\n",
      "keys:loss_objectness:0.3700554072856903\n",
      "keys:loss_rpn_box_reg:0.11994009464979172\n",
      "keys:loss_classifier:0.7428713440895081\n",
      "keys:loss_box_reg:0.5858530402183533\n",
      "keys:loss_objectness:0.2499740719795227\n",
      "keys:loss_rpn_box_reg:0.11753801256418228\n",
      "keys:loss_classifier:0.7198883295059204\n",
      "keys:loss_box_reg:0.47240960597991943\n",
      "keys:loss_objectness:0.2669607698917389\n",
      "keys:loss_rpn_box_reg:0.10446280241012573\n",
      "keys:loss_classifier:0.6501575112342834\n",
      "keys:loss_box_reg:0.37025418877601624\n",
      "keys:loss_objectness:0.21228791773319244\n",
      "keys:loss_rpn_box_reg:0.09480556845664978\n",
      "keys:loss_classifier:0.5474898219108582\n",
      "keys:loss_box_reg:0.35894685983657837\n",
      "keys:loss_objectness:0.184661403298378\n",
      "keys:loss_rpn_box_reg:0.09414089471101761\n",
      "keys:loss_classifier:0.5656149387359619\n",
      "keys:loss_box_reg:0.4252331852912903\n",
      "keys:loss_objectness:0.2263544201850891\n",
      "keys:loss_rpn_box_reg:0.0967058464884758\n",
      "keys:loss_classifier:0.5501576662063599\n",
      "keys:loss_box_reg:0.3228483498096466\n",
      "keys:loss_objectness:0.17781957983970642\n",
      "keys:loss_rpn_box_reg:0.08907650411128998\n",
      "keys:loss_classifier:0.6064589023590088\n",
      "keys:loss_box_reg:0.3391014635562897\n",
      "keys:loss_objectness:0.18389804661273956\n",
      "keys:loss_rpn_box_reg:0.08167217671871185\n",
      "keys:loss_classifier:0.5498131513595581\n",
      "keys:loss_box_reg:0.35080111026763916\n",
      "keys:loss_objectness:0.20476245880126953\n",
      "keys:loss_rpn_box_reg:0.08254764974117279\n",
      "keys:loss_classifier:0.597804069519043\n",
      "keys:loss_box_reg:0.4685494005680084\n",
      "keys:loss_objectness:0.15143196284770966\n",
      "keys:loss_rpn_box_reg:0.08514261245727539\n",
      "keys:loss_classifier:0.6488580703735352\n",
      "keys:loss_box_reg:0.2881052494049072\n",
      "keys:loss_objectness:0.14496353268623352\n",
      "keys:loss_rpn_box_reg:0.07710544019937515\n",
      "keys:loss_classifier:0.5698511600494385\n",
      "keys:loss_box_reg:0.2673216164112091\n",
      "keys:loss_objectness:0.14619532227516174\n",
      "keys:loss_rpn_box_reg:0.07509735226631165\n",
      "keys:loss_classifier:0.4996645748615265\n",
      "keys:loss_box_reg:0.3728310167789459\n",
      "keys:loss_objectness:0.11952182650566101\n",
      "keys:loss_rpn_box_reg:0.07547594606876373\n",
      "keys:loss_classifier:0.45302560925483704\n",
      "keys:loss_box_reg:0.3032253384590149\n",
      "keys:loss_objectness:0.13303175568580627\n",
      "keys:loss_rpn_box_reg:0.07433604449033737\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "num_epochs = 50\n",
    "min_loss = float('inf')\n",
    "pbar = tqdm(range(num_epochs), desc='Initializing')\n",
    "# 初始化损失历史记录\n",
    "loss_histories = {\n",
    "    \"loss_classifier\": [],\n",
    "    \"loss_box_reg\": [],\n",
    "    \"loss_objectness\": [],\n",
    "    \"loss_rpn_box_reg\": []\n",
    "}\n",
    "for epoch in pbar:\n",
    "    # 训练模式\n",
    "    model.train()\n",
    "    train_losses = 0.0\n",
    "    # 移除 torch.cuda.empty_cache() 或在必要时使用\n",
    "    for images, target_dict in train_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v[i].to(device) for k, v in target_dict.items()} for i in range(len(images))]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss_dict = model(images, targets)\n",
    "        # for _keys,_values in train_loss_dict.items():\n",
    "        #     print(f\"keys:{_keys}:{_values}\")\n",
    "\n",
    "        for loss_name in loss_histories.keys():\n",
    "            loss_value = train_loss_dict[loss_name].item()\n",
    "            loss_histories[loss_name].append(loss_value)\n",
    "\n",
    "        current_loss = sum(loss for loss in train_loss_dict.values())\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses += current_loss.item()  # 累加每个批次的损失\n",
    "    train_losses /= len(train_loader)  # 计算平均训练损失\n",
    "    # validate \n",
    "    # val_loss,precision, recall= evaluate(model, val_loader, device)\n",
    "    val_loss,precision, recall= 0.0,0.0,0.0\n",
    "    # 仅在每个 epoch 结束时更新进度条描述\n",
    "    pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_losses:.4f},Val Loss: {val_loss:.4f},Precision:{precision},recall:{recall}')\n",
    "    \n",
    "    # 检查并保存最佳模型\n",
    "    if train_losses < min_loss:\n",
    "        min_loss = train_losses\n",
    "        # 创建保存目录（如果不存在）\n",
    "        save_dir = \"model_weights\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f\"lenet_best_model_epoch_{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "# 训练结束后保存最终模型\n",
    "final_save_path = os.path.join(save_dir, \"lenet_final_model.pth\")\n",
    "torch.save(model.state_dict(), final_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f029865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练结束后绘制损失图表\n",
    "plt.figure(figsize=(12, 8))\n",
    "for loss_name, values in loss_histories.items():\n",
    "    plt.plot(values, label=loss_name)\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7002f",
   "metadata": {},
   "source": [
    "## 可视化展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49da4cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:torch.Size([1, 48, 96])\n",
      "image shape:torch.Size([1, 48, 96])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x4000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# origin version\n",
    "import numpy as np\n",
    "def visualize_predictions(model, val_loader, device, num_images=5):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    images, target_dict = next(iter(val_loader))\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v[i].to(device) for k, v in target_dict.items()} for i in range(len(images))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "    \n",
    "    # 选择要可视化的图像\n",
    "    images = images[:num_images]\n",
    "    predictions = predictions[:num_images]\n",
    "    targets = targets[:num_images]\n",
    "\n",
    "    return images, predictions, targets\n",
    "\n",
    "# debugging\n",
    "train_dataset = HandwrittenDigitsDataset(img_dir='../A+B/samples/inputs', annotations_dir='../A+B/samples/explanations', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 获取一些图像和预测\n",
    "images, predictions, targets = visualize_predictions(model, train_loader, device)\n",
    "def show_image_with_boxes(image, target, prediction, ax):\n",
    "    image = transform(image)\n",
    "    print(f\"image shape:{image.shape}\")\n",
    "    # 将张量图像转换为numpy格式\n",
    "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # 真实边界框\n",
    "    for box in target['boxes'].cpu():\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "            linewidth=2, edgecolor='green', facecolor='none'\n",
    "        )\n",
    "        # print(f\"blue:({box[0]},{box[1]}),({box[2] - box[0]},{box[3] - box[1]})\")\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # 预测边界框\n",
    "    for box in prediction['boxes'].cpu():\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "            linewidth=2, edgecolor='red', facecolor='none'\n",
    "        )\n",
    "        # print(f\"red:({box[0]},{box[1]}),({box[2] - box[0]},{box[3] - box[1]})\")\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(10,40))\n",
    "for img, pred, target, ax in zip(images, predictions, targets, axes):\n",
    "    # 更新 target 和 pred 字典中的每个张量，移动到 CPU\n",
    "    target_cpu = {k: v.cpu() for k, v in target.items()}\n",
    "    pred_cpu = {k: v.cpu() for k, v in pred.items()}\n",
    "    show_image_with_boxes(img.cpu(), target_cpu, pred_cpu, ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f2577c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:tensor([2, 6, 2, 7, 7], device='cuda:2')\n",
      "labels:tensor([7, 2, 2, 2], device='cuda:2')\n",
      "image shape:torch.Size([1, 48, 96])\n",
      "prediction:dict_keys(['boxes', 'scores', 'labels'])\n",
      "image shape:torch.Size([1, 48, 96])\n",
      "prediction:dict_keys(['boxes', 'scores', 'labels'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x4000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 引入了非极大抑制\n",
    "from torchvision.ops import nms\n",
    "def apply_nms(orig_prediction, iou_thresh=0.01):\n",
    "    # 应用 NMS 来过滤边界框\n",
    "    keep = nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
    "    final_prediction = {\n",
    "        'boxes': orig_prediction['boxes'][keep],\n",
    "        'scores': orig_prediction['scores'][keep],\n",
    "        'labels': orig_prediction['labels'][keep] \n",
    "    }\n",
    "    print(f\"labels:{final_prediction['labels']}\")\n",
    "    return final_prediction\n",
    "\n",
    "def visualize_predictions(model, val_loader, device, num_images=2):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    images, target_dict = next(iter(val_loader))\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v[i].to(device) for k, v in target_dict.items()} for i in range(len(images))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "        # 应用 NMS\n",
    "        predictions = [apply_nms(pred) for pred in predictions]\n",
    "    \n",
    "    # 选择要可视化的图像\n",
    "    images = images[:num_images]\n",
    "    predictions = predictions[:num_images]\n",
    "    targets = targets[:num_images]\n",
    "\n",
    "    return images, predictions, targets\n",
    "\n",
    "def show_image_with_boxes(image, target, prediction, ax):\n",
    "    # 将张量图像转换为numpy格式\n",
    "    print(f\"image shape:{image.shape}\")\n",
    "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # 真实边界框和标签\n",
    "    for box, label in zip(target['boxes'].cpu(), target['labels'].cpu()):\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "            linewidth=2, edgecolor='green', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        # 显示类别标签\n",
    "        label_text = f\"{label.item()}\"  # 获取类别标签\n",
    "        ax.text(box[2], box[1], label_text, color='green', verticalalignment='top')\n",
    "\n",
    "    # Debug\n",
    "    print(f\"prediction:{prediction.keys()}\")\n",
    "\n",
    "    # 预测边界框和标签\n",
    "    for idx, (box, label) in enumerate(zip(prediction['boxes'].cpu(), prediction['labels'].cpu())):\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "            linewidth=2, edgecolor='red', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        # 显示类别标签\n",
    "        label_text = f\"{label.item()}\"  # 获取类别标签\n",
    "        ax.text(box[2], box[1], label_text, color='red', verticalalignment='top')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 获取一些图像和预测\n",
    "images, predictions, targets = visualize_predictions(model, train_loader, device,num_images=2)\n",
    "# 绘制图像和预测\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(10,40))\n",
    "for img, pred, target, ax in zip(images, predictions, targets, axes):\n",
    "    target_cpu = {k: v.cpu() for k, v in target.items()}\n",
    "    pred_cpu = {k: v.cpu() for k, v in pred.items()}\n",
    "    show_image_with_boxes(img.cpu(), target_cpu, pred_cpu, ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504179bf",
   "metadata": {},
   "source": [
    "# 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_names = [img for img in os.listdir(img_dir) if img.endswith('.jpg')]\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        image = read_image(img_path).float()  # 直接转换为浮点类型\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "    \n",
    "# 定义测试集的转换\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((48, 96)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# 创建测试集\n",
    "test_dataset = TestDataset(img_dir='A+B/inputs', transform=test_transform)\n",
    "\n",
    "# 创建测试集 DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66e60f",
   "metadata": {},
   "source": [
    "## 提交格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52314bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保输出文件夹存在，\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model.eval()  # 设置模型为评估模式\n",
    "with torch.no_grad():\n",
    "    for i, images in enumerate(test_loader, start=0):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 获取预测结果\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # 将预测结果保存到文本文件\n",
    "        for idx, pred in enumerate(predicted, start=1):\n",
    "            with open(os.path.join(output_dir, f'{i*test_loader.batch_size + idx}.txt'), 'w') as file:\n",
    "                file.write(str(pred.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514b5f5",
   "metadata": {},
   "source": [
    "## 观测模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70939d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# 确保输出文件夹存在\n",
    "output_dir = '.'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# CSV 文件路径\n",
    "csv_file_path = os.path.join(output_dir, 'predictions.csv')\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 创建CSV文件并写入预测结果\n",
    "with torch.no_grad(), open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Image_Index', 'Predicted_Sum'])\n",
    "\n",
    "    for i, images in enumerate(test_loader, start=0):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 获取预测结果\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # 写入每个预测结果\n",
    "        for idx, pred in enumerate(predicted, start=1):\n",
    "            image_index = i*test_loader.batch_size + idx\n",
    "            writer.writerow([image_index, pred.item()])\n",
    "\n",
    "print(f'Predictions saved to {csv_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4a1e9",
   "metadata": {},
   "source": [
    "## test set 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6762b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置展示图片的数量\n",
    "num_images_to_show = 3\n",
    "\n",
    "# 确保模型处于评估模式\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 从测试集中获取一些图像\n",
    "    images = next(iter(test_loader))[:num_images_to_show]\n",
    "    images = images.to(device)\n",
    "    outputs = model(images)\n",
    "\n",
    "    # 获取预测结果\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # 将图像转移到CPU并转换为numpy数组\n",
    "    images = images.cpu().numpy()\n",
    "\n",
    "# 绘制图像和预测结果\n",
    "for i in range(num_images_to_show):\n",
    "    plt.subplot(1, num_images_to_show, i+1)\n",
    "    plt.imshow(np.squeeze(images[i]), cmap='gray')\n",
    "    plt.title(f'Predicted: {predicted[i]}')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c9b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
